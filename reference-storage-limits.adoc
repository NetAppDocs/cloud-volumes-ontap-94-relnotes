---
sidebar: sidebar
permalink: reference-storage-limits.html
keywords: limits, maximum, storage, aggregates, disks, volumes, capacity, luns, size, storage virtual machine, SVM
summary: Cloud Volumes ONTAP has storage configuration limits to provide reliable operations. For best performance, do not configure your system at the maximum values.
---

= Storage limits
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
Cloud Volumes ONTAP has storage configuration limits to provide reliable operations. For best performance, do not configure your system at the maximum values.

== Maximum system capacity by license

The maximum system capacity for a Cloud Volumes ONTAP system is determined by its license. The maximum system capacity includes disk-based storage plus object storage used for data tiering. NetApp doesn't support exceeding this limit.

In Azure, disk limits prevent you from reaching the 368 TiB capacity limit by using disks alone. In those cases, you can reach the 368 TiB capacity limit by https://docs.netapp.com/us-en/occm/concept_data_tiering.html[tiering inactive data to object storage^]. Refer to <<Capacity and disk limits by Azure VM size,capacity and disk limits by Azure VM size>> for more details.

[cols="30,70",width=60%,options="header"]
|===
| License
| Maximum system capacity (disks + object storage)

| Explore	| 2 TiB (data tiering is not supported with Explore)
| Standard | 10 TiB
| Premium | 368 TiB
| BYOL | 368 TiB per license

|===

===== For HA, is the license capacity limit per node or for the entire HA pair?

The capacity limit is for the entire HA pair. It is not per node. For example, if you use the Premium license, you can have up to 368 TiB of capacity between both nodes.

===== For an HA system in AWS, does mirrored data count against the capacity limit?

No, it doesn't. Data in an AWS HA pair is synchronously mirrored between the nodes so that the data is available in the event of failure. For example, if you purchase an 8 TiB disk on node A, Cloud Manager also allocates an 8 TiB disk on node B that is used for mirrored data. While 16 TiB of capacity was provisioned, only 8 TiB counts against the license limit.

== Aggregate and disk limits for Cloud Volumes ONTAP in AWS

In Cloud Volumes ONTAP 9.4, all EC2 instance types can reach the 368 TiB capacity limit using EBS storage alone, or by using EBS storage and tiering to S3 (both single node and HA).

[cols=3*,options="header,autowidth"]
|===
| Physical storage
| Parameter
| Limit

.5+| *Aggregates and disks*
| Maximum number of aggregates | 34 for single-node configurations
18 per node in an HA configuration ^1^
| Maximum aggregate size |	96 TiB of raw capacity ^2^
| Disks per aggregate	| 1-6 ^3^
| Maximum disk size | 16 TiB
| Maximum number of data disks across all aggregates ^4^ | 34 for single-node configurations
31 per node in an HA configuration
| *RAID groups*	| Maximum per aggregate	| 1
|===

Notes:

. It is not possible to create 18 aggregates on both nodes in an HA pair because doing so would exceed the data disk limit.

. The aggregate capacity limit is based on the disks that comprise the aggregate. The limit does not include object storage used for data tiering.

. All disks in an aggregate must be the same size.

. The data disk limit is specific to disks that contain user data. The boot disk and root disk for each node are not included in this limit.

== Aggregate and disk limits for Cloud Volumes ONTAP in Azure

[cols=3*,options="header,autowidth"]
|===
| Physical storage
| Parameter
| Limit

.5+| *Aggregates and disks*
| Maximum number of aggregates | Same as the disk limit
| Maximum aggregate size |	200 TiB of raw capacity ^1^
| Disks per aggregate	| 1-12 ^2^
| Maximum disk size | 32 TiB
| Maximum number of data disks across all aggregates ^3^ | Depends on VM size. <<Capacity and disk limits by Azure VM size,See below>>.
| *RAID groups*	| Maximum per aggregate	| 1
|===

Notes:

. The aggregate capacity limit is based on the disks that comprise the aggregate. The limit does not include object storage used for data tiering.

. All disks in an aggregate must be the same size.

. The data disk limit is specific to disks that contain user data. The boot disk and root disk for each node are not included in this limit.

=== Capacity and disk limits by Azure VM size

In Azure, single node systems can use Standard HDD Managed Disks, Standard SSD Managed Disks, and Premium SSD Managed Disks, with up to 32 TiB per disk. The number of supported disks varies by VM size.

The tables below show the maximum system capacity by VM size with disks alone, and with disks and cold data tiering to object storage.

Disk limits are shown by VM size for Premium and BYOL licenses only because disk limits can’t be reached with Explore or Standard licenses due to system capacity limits.

==== Single node with a Premium license

[cols="14,20,31,33",width=68%,options="header"]
|===
| VM size
| Max disks per node
| Max system capacity with disks alone
| Max system capacity with disks and data tiering

| DS3_v2 | 15 | 368 TiB | Tiering not supported
| DS4_v2 | 31 | 368 TiB | 368 TiB
| DS5_v2 | 63 | 368 TiB | 368 TiB
| DS13_v2 | 31 | 368 TiB | 368 TiB
| DS14_v2 | 63 | 368 TiB | 368 TiB
|===

==== Single node with one or more BYOL licenses

NOTE: For some VM types, you'll need several BYOL licenses to reach the max system capacity listed below. For example, you'd need 6 BYOL licenses to reach 2 PiB with DS5_v2.

[cols="10,18,18,18,18,18",width=100%,options="header"]
|===
| VM size
| Max disks per node
2+| Max system capacity with one license
2+| Max system capacity with multiple licenses

2+| | *Disks alone* | *Disks + data tiering* | *Disks alone* | *Disks + data tiering*

| DS3_v2 | 15 | 368 TiB | Tiering not supported | 480 TiB | Tiering not supported
| DS4_v2 | 31 | 368 TiB | 368 TiB | 992 TiB | 368 TiB x each license
| DS5_v2 | 63 | 368 TiB | 368 TiB | 2 PiB | 368 TiB x each license
| DS13_v2 | 31 | 368 TiB | 368 TiB | 992 TiB | 368 TiB x each license
| DS14_v2 | 63 | 368 TiB | 368 TiB | 2 PiB | 368 TiB x each license
|===


== Logical storage limits

[cols="22,22,56",width=100%,options="header"]
|===
| Logical storage
| Parameter
| Limit

| *Storage virtual machines (SVMs)*	| Maximum number for Cloud Volumes ONTAP
(HA pair or single node) | One data-serving SVM and one destination SVM used for disaster recovery. You can activate the destination SVM for data access if there’s an outage on the source SVM. ^1^

The one data-serving SVM spans the entire Cloud Volumes ONTAP system (HA pair or single node).
.2+| *Files*	| Maximum size | 16 TiB
| Maximum per volume |	Volume size dependent, up to 2 billion
| *FlexClone volumes*	| Hierarchical clone depth ^2^ | 499
.3+| *FlexVol volumes*	| Maximum per node |	500
| Minimum size |	20 MB
| Maximum size |	AWS: Dependent on the size of the aggregate ^3^
Azure: 100 TiB
| *Qtrees* |	Maximum per FlexVol volume |	4,995
| *Snapshot copies* |	Maximum per FlexVol volume |	1,023

|===

Notes:

. Cloud Manager does not provide any setup or orchestration support for SVM disaster recovery. It also does not support storage-related tasks on an additional SVM. You must use System Manager or the CLI for SVM disaster recovery.
+
* https://library.netapp.com/ecm/ecm_get_file/ECMLP2839856[SVM Disaster Recovery Preparation Express Guide^]
* https://library.netapp.com/ecm/ecm_get_file/ECMLP2839857[SVM Disaster Recovery Express Guide^]

. Hierarchical clone depth is the maximum depth of a nested hierarchy of FlexClone volumes that can be created from a single FlexVol volume.

. Less than 100 TiB is supported because aggregates for this configuration are limited to 96 TiB of _raw_ capacity.

== iSCSI storage limits

[cols=3*,options="header,autowidth"]
|===
| iSCSI storage
| Parameter
| Limit

.4+| *LUNs*	| Maximum per node |	1,024
| Maximum number of LUN maps |	1,024
| Maximum size	| 16 TiB
| Maximum per volume	| 512
| *igroups*	| Maximum per node | 256
.2+| *Initiators*	| Maximum per node |	512
| Maximum per igroup	| 128
| *iSCSI sessions* |	Maximum per node | 1,024
.2+| *LIFs*	| Maximum per port |	32
| Maximum per portset	| 32
| *Portsets* |	Maximum per node |	256

|===
